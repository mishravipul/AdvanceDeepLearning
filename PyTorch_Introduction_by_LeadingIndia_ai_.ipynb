{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Introduction by LeadingIndia.ai .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs3raTEsRISM",
        "colab_type": "text"
      },
      "source": [
        "<center>\n",
        "\n",
        "  ![alt text](https://www.leadingindia.ai/images/newimages/Leading-india-logo.png)\n",
        "</center>\n",
        "\n",
        "# 1. Introduction\n",
        "\n",
        "\n",
        "*   PyTorch is a deep learning research platform that provides maximum flexibility and speed\n",
        "*   It was developed by **Facebook** and is based on now obsolete Torch\n",
        "* PyTorch also provides tensor computation (like NumPy) with strong GPU acceleration\n",
        "* PyTorch deals extensively in Tensors\n",
        "* Tensors act as <font color=red><b>fundamental data structure</b></font>\n",
        "* PyTorch Documentation - https://pytorch.org/docs/stable/index.html\n",
        "* For learning more about AI, Machine Learning, Deep Learning and Deep Reinforcement Learning - www.leadingindia.ai\n",
        "\n",
        "\n",
        "Importing and checking version of PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0tUjAj8Rmez",
        "colab_type": "code",
        "outputId": "b352a7c0-f010-4036-fd86-7c691b4c16da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      },
      "source": [
        "import torch #to import PyTorch\n",
        "print('PyTorch version:', torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version: 1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "902_ZwoYrXAq",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Understanding Tensors in PyTorch\n",
        "\n",
        "\n",
        "\n",
        "*   PyTorch Tensors behave very similar to NumPy Arrays\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Creating a random tensor of m*n dimensions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olaj31H2qx6W",
        "colab_type": "code",
        "outputId": "bc54bfb4-9552-4d97-f5e5-fc11288c2610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "x=torch.rand(5,4)#m=5,n=6\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8581, 0.2606, 0.8795, 0.3583],\n",
            "        [0.4512, 0.3856, 0.5904, 0.5332],\n",
            "        [0.5334, 0.7254, 0.2995, 0.3103],\n",
            "        [0.3927, 0.6992, 0.0224, 0.3235],\n",
            "        [0.1703, 0.8683, 0.6911, 0.0416]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS1hoz0sr4mQ",
        "colab_type": "text"
      },
      "source": [
        "**To find size of the tensor use size() function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACQB24LDro_B",
        "colab_type": "code",
        "outputId": "9dd6693e-f3f1-49ce-c8d0-d187a4361f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "print(x.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWKpL_I0s8YF",
        "colab_type": "text"
      },
      "source": [
        "**Creating a Tensor consisting of ones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4-ydFQ5r4Nm",
        "colab_type": "code",
        "outputId": "a5747a45-1b57-4cbb-b450-5bf065b91570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "y=torch.ones(5,4)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSlQw9o7tWgD",
        "colab_type": "text"
      },
      "source": [
        "**Adding two Tensors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns9K0MDJtA1l",
        "colab_type": "code",
        "outputId": "8f195a5e-0543-4cc4-9ca6-d6fa91de8b99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "z=x+y\n",
        "print(z)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.8581, 1.2606, 1.8795, 1.3583],\n",
            "        [1.4512, 1.3856, 1.5904, 1.5332],\n",
            "        [1.5334, 1.7254, 1.2995, 1.3103],\n",
            "        [1.3927, 1.6992, 1.0224, 1.3235],\n",
            "        [1.1703, 1.8683, 1.6911, 1.0416]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TNX-GAIttJu",
        "colab_type": "text"
      },
      "source": [
        "**Getting Rows and Columns of Tensors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er2NvsCctZtR",
        "colab_type": "code",
        "outputId": "85d323ba-cfdd-44f7-b21c-d67a2bb7503c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "print(x[1])#to get first row of Tensor x\n",
        "print(x[:,2])#to get  2nd column of Tensor x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.4512, 0.3856, 0.5904, 0.5332])\n",
            "tensor([0.8795, 0.5904, 0.2995, 0.0224, 0.6911])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gy2wVO7ihBD",
        "colab_type": "text"
      },
      "source": [
        "**There are two types of methods associated with Tensors:**\n",
        "\n",
        "\n",
        "\n",
        "1.   Methods returning a copy and not modifying original Tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnhzHNh_imjf",
        "colab_type": "code",
        "outputId": "fa2495cd-3762-484f-a588-b92a772a5724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "x.add(1)#will not modify original Tensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.8581, 1.2606, 1.8795, 1.3583],\n",
              "        [1.4512, 1.3856, 1.5904, 1.5332],\n",
              "        [1.5334, 1.7254, 1.2995, 1.3103],\n",
              "        [1.3927, 1.6992, 1.0224, 1.3235],\n",
              "        [1.1703, 1.8683, 1.6911, 1.0416]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5XKWOCNi-x4",
        "colab_type": "code",
        "outputId": "a4891a1f-61b1-44ac-c016-3bb2ba0c2acf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8581, 0.2606, 0.8795, 0.3583],\n",
            "        [0.4512, 0.3856, 0.5904, 0.5332],\n",
            "        [0.5334, 0.7254, 0.2995, 0.3103],\n",
            "        [0.3927, 0.6992, 0.0224, 0.3235],\n",
            "        [0.1703, 0.8683, 0.6911, 0.0416]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p1WOWTPjJAN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "2.   In place version of methods that modify the original Tensor (normally ends with _)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3s0tegBjFBL",
        "colab_type": "code",
        "outputId": "2367544c-4b33-4b9f-f7c0-c3221452fde3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "x.add_(1)#will modify original Tensor x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.8581, 1.2606, 1.8795, 1.3583],\n",
              "        [1.4512, 1.3856, 1.5904, 1.5332],\n",
              "        [1.5334, 1.7254, 1.2995, 1.3103],\n",
              "        [1.3927, 1.6992, 1.0224, 1.3235],\n",
              "        [1.1703, 1.8683, 1.6911, 1.0416]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufRQHeisjRIL",
        "colab_type": "code",
        "outputId": "c86d0732-28cd-4c94-d06a-d2246ee8b031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.8581, 1.2606, 1.8795, 1.3583],\n",
            "        [1.4512, 1.3856, 1.5904, 1.5332],\n",
            "        [1.5334, 1.7254, 1.2995, 1.3103],\n",
            "        [1.3927, 1.6992, 1.0224, 1.3235],\n",
            "        [1.1703, 1.8683, 1.6911, 1.0416]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts72e3lAj2Xv",
        "colab_type": "text"
      },
      "source": [
        "**Getting shape and reshaping the Tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI35UPm9jZHf",
        "colab_type": "code",
        "outputId": "c33768db-3af8-4559-c8b5-1287a4676bb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print(x)\n",
        "print(x.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.8581, 1.2606, 1.8795, 1.3583],\n",
            "        [1.4512, 1.3856, 1.5904, 1.5332],\n",
            "        [1.5334, 1.7254, 1.2995, 1.3103],\n",
            "        [1.3927, 1.6992, 1.0224, 1.3235],\n",
            "        [1.1703, 1.8683, 1.6911, 1.0416]])\n",
            "torch.Size([5, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Zejq7yj-E1",
        "colab_type": "code",
        "outputId": "5fa886a9-a11c-4265-bfaf-a1451ab5d817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "x.resize_(4,5)#reshaping Tensor to 4*5\n",
        "print(x)\n",
        "print(x.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.8581, 1.2606, 1.8795, 1.3583, 1.4512],\n",
            "        [1.3856, 1.5904, 1.5332, 1.5334, 1.7254],\n",
            "        [1.2995, 1.3103, 1.3927, 1.6992, 1.0224],\n",
            "        [1.3235, 1.1703, 1.8683, 1.6911, 1.0416]])\n",
            "torch.Size([4, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg4fOcbrlkqv",
        "colab_type": "text"
      },
      "source": [
        "**Converting a NumPy array to PyTorch Tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojuMkjFGkHdU",
        "colab_type": "code",
        "outputId": "07d33508-7d5e-429d-9598-7f591995e516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        }
      },
      "source": [
        "import numpy as np\n",
        "a = np.random.rand(3,4)#creating a random numpy array of dimension 3*4\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.13251852 0.74202953 0.87060535 0.07415182]\n",
            " [0.79307267 0.80280482 0.06018786 0.57913947]\n",
            " [0.97375275 0.84528076 0.16455167 0.39917798]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBdWElhTmEP0",
        "colab_type": "code",
        "outputId": "aec36ea1-f697-467e-edca-786703df8601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        }
      },
      "source": [
        "b = torch.from_numpy(a)#converting numpy array into Torch tensor\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1325, 0.7420, 0.8706, 0.0742],\n",
            "        [0.7931, 0.8028, 0.0602, 0.5791],\n",
            "        [0.9738, 0.8453, 0.1646, 0.3992]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td676isEmpQg",
        "colab_type": "text"
      },
      "source": [
        "**Convering Torch Tensor to Numpy array**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJUSyc5Hmgck",
        "colab_type": "code",
        "outputId": "02589b56-3a62-45fd-f56f-f27b066f83fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "c = b.numpy()\n",
        "print(type(c))\n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[[0.13251852 0.74202953 0.87060535 0.07415182]\n",
            " [0.79307267 0.80280482 0.06018786 0.57913947]\n",
            " [0.97375275 0.84528076 0.16455167 0.39917798]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZldRyt6BnO2M",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>**Note :** </font>NumPy array and PyTorch tensors share memory. Thus if one is modifies other will automatically get updated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuLz-uhQnACL",
        "colab_type": "code",
        "outputId": "f8e0ed8d-b0b6-4a1e-b357-30a5d3730da2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "b[0,1] = 9 #modifying tensor b\n",
        "print(b)\n",
        "print(c) #NumPy array c which was created from PyTorch tensor b will automatically get updated"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1325, 9.0000, 0.8706, 0.0742],\n",
            "        [0.7931, 0.8028, 0.0602, 0.5791],\n",
            "        [0.9738, 0.8453, 0.1646, 0.3992]], dtype=torch.float64)\n",
            "[[0.13251852 9.         0.87060535 0.07415182]\n",
            " [0.79307267 0.80280482 0.06018786 0.57913947]\n",
            " [0.97375275 0.84528076 0.16455167 0.39917798]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AhFkj70oMRg",
        "colab_type": "text"
      },
      "source": [
        "# 2. Deep Learning using PyTorch\n",
        "\n",
        "\n",
        "*   We will be using **[TORCHVISION](https://pytorch.org/docs/stable/torchvision/index.html)** which contains many of popular dataset readily available to use in PyTorch\n",
        "* As an introduction we will start with **[MNIST](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist)** Dataset\n",
        "\n",
        "## 2.1 Building the Neural Network\n",
        "**Importing the necessary packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PABioUvmnt-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch #base library\n",
        "\n",
        "import helper #to visually analyze the output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms #for importing datasets and applying some transformations on those datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxzHqMQpslWd",
        "colab_type": "text"
      },
      "source": [
        "**Downloading, tranforming and splitting the MNIST dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqMsSB7WsvNl",
        "colab_type": "code",
        "outputId": "e1815e37-8b98-4891-d42c-8b05aa1500ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5, ), (0.5,)),# Subtract 0.5 from each pixel value and then divide it by 0.5 so as convert range of pixel value form -1 to +1\n",
        "                             ])\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3670196.31it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 48941.24it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:02, 817123.56it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 18343.12it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdtuYvyeuVwZ",
        "colab_type": "text"
      },
      "source": [
        "Goto Files by selecting left menu <font size=5> ▶️</font> and select refresh and then ensure that MNIST dataset is actually saved on the disk\n",
        "\n",
        "**Ensuring that dataset has loaded correctly**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIXLF3bNuO8-",
        "colab_type": "code",
        "outputId": "a4aca110-ddfb-48f6-9038-a42886cad4fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "plt.imshow(images[8].numpy().squeeze(),cmap='Greys_r')#display the 8th image in the batch\n",
        "print(labels[8].numpy())#display the label of 8th image in the batch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r718y5M98tn6",
        "colab_type": "text"
      },
      "source": [
        "**Details of the network that we will be building**\n",
        "\n",
        "\n",
        "\n",
        "*   Input Layer 784 units\n",
        "*   Hidden Layer 1 with 128 units having ReLU activation\n",
        "*  Hidden Layer 2 with 64 units having ReLU activation\n",
        "* Output Layer with 10 units having Softmax activation for classification of digits\n",
        "* The Loss function will be cross entropy loss\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPqupfEOwzS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn #to build neural network\n",
        "from torch import optim #for optimization algorithms like ADAM, RMSProp\n",
        "import torch.nn.functional as F #many neural network functions such as relu, softmax, convolution etc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAAV2aIJA9zu",
        "colab_type": "text"
      },
      "source": [
        "**To create Deep Neural Networks, generally we have to create a python class that is subclass of nn.Module**\n",
        "\n",
        "We have two options:\n",
        "\n",
        "\n",
        "1.   Create a subclass of nn.Module and specify the network architecture\n",
        "2.   Using an auxillary module nn.Sequential() to build a model where tensor will be passed from one layer to other sequentially\n",
        "\n",
        "We can use either one of them.\n",
        "\n",
        "<font color=red>**Option 1:**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaiIBbov_Tlq",
        "colab_type": "code",
        "outputId": "a2a2f8db-95bf-4d09-f945-b7fab39a914e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "class Network(nn.Module):#specifying our network architecture\n",
        "  def __init__(self): #initializing the network\n",
        "    super().__init__()#it will call the init method of nn.Module\n",
        "    self.fc1 = nn.Linear(784,128)\n",
        "    self.fc2 = nn.Linear(128,64)\n",
        "    self.fc3 = nn.Linear(64,10)\n",
        "  \n",
        "  #all PyTorch Network must have forward method representing forward pass\n",
        "  def forward(self,x):#x is a tensor passed to forward function\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.softmax(x, dim=1)#The tensor will be of size 64(batcg size) * 10; we want to apply softmax to secomd dimension\n",
        "    return x   \n",
        "model = Network()\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd9GmCr2gk_M",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>**Option 2:**</font> **Creating a Sequential Model to pass tensors from one layer to the other sequentially using <font color=red>nn.Sequential</font>**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ39u6ksgm-7",
        "colab_type": "code",
        "outputId": "5c52e075-be50-4cbd-9700-1bc0daed7e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "input_size = 784\n",
        "hidden_sizes = [128, 64]\n",
        "output_size = 10\n",
        "\n",
        "# Build a feed-forward network\n",
        "modelSeq = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      nn.Softmax(dim=1))\n",
        "print(modelSeq)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (5): Softmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1LPY-bwTA1E",
        "colab_type": "text"
      },
      "source": [
        "We will be using option 1 to create Neural Network.\n",
        "\n",
        "**Weights and bias are automatically initialized for us for each layer and we can display them as:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOs3IdizNkKF",
        "colab_type": "code",
        "outputId": "2c1c7ac8-88cf-4b5b-b22f-327fc93c33e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "source": [
        "print(model.fc1.weight)\n",
        "print(model.fc1.bias)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0091,  0.0021, -0.0284,  ...,  0.0037,  0.0350,  0.0130],\n",
            "        [ 0.0121, -0.0150,  0.0158,  ..., -0.0157,  0.0108,  0.0198],\n",
            "        [ 0.0177, -0.0208,  0.0083,  ...,  0.0173, -0.0033, -0.0304],\n",
            "        ...,\n",
            "        [-0.0081, -0.0257,  0.0196,  ...,  0.0078,  0.0176,  0.0235],\n",
            "        [ 0.0164,  0.0117,  0.0331,  ...,  0.0100, -0.0210,  0.0028],\n",
            "        [ 0.0135, -0.0134,  0.0229,  ..., -0.0241,  0.0114, -0.0335]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0261, -0.0221,  0.0348,  0.0325,  0.0180, -0.0326, -0.0154, -0.0303,\n",
            "        -0.0313,  0.0190,  0.0112, -0.0324, -0.0323, -0.0289,  0.0164,  0.0157,\n",
            "        -0.0325, -0.0083,  0.0145,  0.0335,  0.0213,  0.0192, -0.0252,  0.0120,\n",
            "        -0.0039,  0.0122,  0.0081, -0.0238, -0.0083, -0.0132, -0.0290, -0.0072,\n",
            "         0.0092, -0.0248, -0.0161,  0.0171,  0.0039,  0.0092,  0.0304, -0.0235,\n",
            "         0.0204,  0.0120, -0.0266,  0.0194, -0.0050, -0.0005,  0.0344, -0.0004,\n",
            "         0.0144,  0.0339, -0.0134, -0.0128, -0.0025, -0.0108, -0.0190,  0.0094,\n",
            "        -0.0091,  0.0271, -0.0286,  0.0151,  0.0246,  0.0048,  0.0014,  0.0345,\n",
            "         0.0066,  0.0177,  0.0178,  0.0060,  0.0297, -0.0190, -0.0331, -0.0257,\n",
            "         0.0058, -0.0269,  0.0072, -0.0306, -0.0101,  0.0109, -0.0333,  0.0157,\n",
            "         0.0126, -0.0236, -0.0070, -0.0153, -0.0015, -0.0340,  0.0181,  0.0273,\n",
            "         0.0099, -0.0162,  0.0086, -0.0104, -0.0199,  0.0094, -0.0285, -0.0015,\n",
            "        -0.0056, -0.0234,  0.0128,  0.0087,  0.0104, -0.0215, -0.0272, -0.0336,\n",
            "         0.0203,  0.0240, -0.0223, -0.0057,  0.0048,  0.0186,  0.0049, -0.0153,\n",
            "         0.0124,  0.0014, -0.0276, -0.0098,  0.0108, -0.0066,  0.0204, -0.0020,\n",
            "         0.0127,  0.0301, -0.0177,  0.0217, -0.0047, -0.0226, -0.0017, -0.0346],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuCFDThQmFRE",
        "colab_type": "text"
      },
      "source": [
        "### Let us see the output of our untrained Neural Network\n",
        "\n",
        "**Forward Pass:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTtL_e7iTYF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images , labels = next(iter(trainloader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTWYjrfwmtDk",
        "colab_type": "text"
      },
      "source": [
        "**Flattening the images to load in Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lrPJZIKmpe1",
        "colab_type": "code",
        "outputId": "bea3ee66-56c8-4ca1-b266-10772124aacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "batch_size = images.shape[0] #64 is batch size\n",
        "images.resize_(batch_size, 784)#in_place resizing of image\n",
        "print(images.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxHpn5VKvVdc",
        "colab_type": "text"
      },
      "source": [
        "**Checking the predictions of untrained network:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh6-wU_ivqHW",
        "colab_type": "code",
        "outputId": "c0c725ac-6e64-4af1-c0a5-4cf844ee3b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "prob = model.forward(images)#classification probabilities based on untrained model\n",
        "print('Predicted Label:',np.argmax(prob[1].data.numpy()))\n",
        "print('Actual Label:', labels[1].numpy())#actual label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Label: 5\n",
            "Actual Label: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jilDR8B3_DWK",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Training our Neural Network\n",
        "\n",
        "\n",
        "*   Define our neural network's loss function\n",
        "*   Define the backpropagation or optimization algorithm like SGD or ADAM\n",
        "*  To calculate gradient PyTorch uses a module called **[autograd](https://pytorch.org/docs/stable/autograd.html)**\n",
        "      * autograd keeps track of all the operations performed on a Tensor\n",
        "      * We need to set **requires_grad** on a tensor\n",
        "      *  We can do this ar creation with the requires_grad keyword, or at any time with **x.requires_grad_(True).**\n",
        "      * To compute the gradient, we need to call **backward()** method on the Tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dZUo7cBofbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lossCriteria = nn.CrossEntropyLoss() #defining loss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005) #lr is learning rate "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCVx4O26JfWR",
        "colab_type": "text"
      },
      "source": [
        "General process in PyTorch to update weights\n",
        "* Make a forward pass through the network to get the results/logits\n",
        "* Use the logits to calculate the loss\n",
        "* Perform a backward pass through the network with **loss.backward()** to calculate the gradients\n",
        "* Take a step with the optimizer to update the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNtzw_jyH8jU",
        "colab_type": "code",
        "outputId": "9a3e2cc4-f9a3-41c5-ef9a-4af1b71b62ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "print('Weights Before Optimization ', model.fc1.weight)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights Before Optimization  Parameter containing:\n",
            "tensor([[-0.0091,  0.0021, -0.0284,  ...,  0.0037,  0.0350,  0.0130],\n",
            "        [ 0.0121, -0.0150,  0.0158,  ..., -0.0157,  0.0108,  0.0198],\n",
            "        [ 0.0177, -0.0208,  0.0083,  ...,  0.0173, -0.0033, -0.0304],\n",
            "        ...,\n",
            "        [-0.0081, -0.0257,  0.0196,  ...,  0.0078,  0.0176,  0.0235],\n",
            "        [ 0.0164,  0.0117,  0.0331,  ...,  0.0100, -0.0210,  0.0028],\n",
            "        [ 0.0135, -0.0134,  0.0229,  ..., -0.0241,  0.0114, -0.0335]],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMDdAlJyLFkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "print_every = 40 #to print the current loss after every 40 steps\n",
        "steps = 0\n",
        "for e in range(epochs):\n",
        "    running_loss = 0 #total loss till in previous 'print_every'steps\n",
        "    for images, labels in iter(trainloader):\n",
        "        steps += 1\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images.resize_(images.shape[0], 784)\n",
        "        \n",
        "        optimizer.zero_grad()#!!!VERY IMPORTANT. To ensure for each backpropagation, new gradients are computed\n",
        "        \n",
        "        # Forward and backward passes\n",
        "        output = model.forward(images)\n",
        "        loss = lossCriteria(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()#update weights\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if steps % print_every == 0:\n",
        "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
        "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
        "            \n",
        "            running_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfX0ofD4W2Pn",
        "colab_type": "code",
        "outputId": "e694cf17-3bcd-4f6d-e048-344bedd56355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "print('Weights After Optimization ', model.fc1.weight)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights After Optimization  Parameter containing:\n",
            "tensor([[-0.0116, -0.0003, -0.0309,  ...,  0.0013,  0.0325,  0.0106],\n",
            "        [ 0.0120, -0.0151,  0.0157,  ..., -0.0158,  0.0107,  0.0197],\n",
            "        [ 0.0179, -0.0205,  0.0086,  ...,  0.0175, -0.0030, -0.0302],\n",
            "        ...,\n",
            "        [-0.0079, -0.0255,  0.0198,  ...,  0.0080,  0.0178,  0.0236],\n",
            "        [ 0.0164,  0.0117,  0.0331,  ...,  0.0100, -0.0211,  0.0028],\n",
            "        [ 0.0140, -0.0129,  0.0234,  ..., -0.0236,  0.0119, -0.0330]],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xa9f9WQJOwz",
        "colab_type": "text"
      },
      "source": [
        "**Checking the predictions of trained network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJmC0QKlJebv",
        "colab_type": "code",
        "outputId": "1d08a9d3-adf3-47ec-809e-7f23ef321586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        }
      },
      "source": [
        "model.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(testloader))\n",
        "batch_size = images.shape[0] #64 is batch size\n",
        "images.resize_(batch_size, 784)#in_place resizing of image\n",
        "print(images.size())\n",
        "output = model.forward(images)#classification probabilities based on untrained model\n",
        "print('Predicted Label:',np.argmax(output[1].data.numpy()))\n",
        "print('Actual Label:', labels[1].numpy())#actual label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 784])\n",
            "Predicted Label: 1\n",
            "Actual Label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPXPQSZqmPYL",
        "colab_type": "text"
      },
      "source": [
        "## Challenge 2.1  \n",
        "Build a network to classify the MNIST images with three hidden layers **using option 2** as discussed above. Use 400 units in the first hidden layer, 200 units in the second layer, and 100 units in the third layer. Each hidden layer should have a ReLU activation function, and use softmax on the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awkJ8VrCPfG4",
        "colab_type": "code",
        "outputId": "79c3d661-cc2a-4728-f5f3-859f578d851b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "#Make changes in this cell\n",
        "#dataset aleady loaded\n",
        "#all required modules are already imported\n",
        "input_size = 784\n",
        "hidden_sizes = ['Fix_Me','Fix_Me',\"Fix_Me\"]\n",
        "output_size = 10\n",
        "\n",
        "# Build a feed-forward network\n",
        "#Fix Me - Add layers here. Take help from above cells\n",
        "model3 = nn.Sequential(nn.Linear(input_size, 'Fix_Me'),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear('Fix_Me', 'Fix_Me'),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear('Fix_Me', 'Fix_Me'),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear('Fix_Me',output_size),\n",
        "                      nn.Softmax(dim=1))\n",
        "print(model3)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-45bbe8c11153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Build a feed-forward network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Fix Me - Add layers here. Take help from above cells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m model3 = nn.Sequential(nn.Linear(input_size, 'Fix_Me'),\n\u001b[0m\u001b[1;32m      8\u001b[0m                       \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                       \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fix_Me'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Fix_Me'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (str, int), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the arguments have invalid types: (!str!, !int!)\n * (object data, torch.device device)\n      didn't match because some of the arguments have invalid types: (!str!, !int!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPEesvrqJNjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Do not change anything in this cell\n",
        "lossCriteria = nn.CrossEntropyLoss() #defining loss\n",
        "optimizer = torch.optim.SGD(model3.parameters(), lr=0.005) #lr is learning rate \n",
        "epochs = 10\n",
        "print_every = 40 #to print the current loss after every 40 steps\n",
        "steps = 0\n",
        "for e in range(epochs):\n",
        "    running_loss = 0 #total loss till in previous 'print_every'steps\n",
        "    for images, labels in iter(trainloader):\n",
        "        steps += 1\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images.resize_(images.shape[0], 784)\n",
        "        \n",
        "        optimizer.zero_grad()#!!!VERY IMPORTANT. To ensure for each backpropagation, new gradients are computed\n",
        "        \n",
        "        # Forward and backward passes\n",
        "        output = model3.forward(images)\n",
        "        loss = lossCriteria(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()#update weights\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if steps % print_every == 0:\n",
        "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
        "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
        "            \n",
        "            running_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ-6tDEVVD7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Do not change anything in this cell\n",
        "#Testing predictions of trained model\n",
        "model3.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(testloader))\n",
        "batch_size = images.shape[0] #64 is batch size\n",
        "images.resize_(batch_size, 784)#in_place resizing of image\n",
        "print(images.size())\n",
        "prob = model3.forward(images)#classification probabilities based on untrained model\n",
        "print('Predicted Label:',np.argmax(prob[1].data.numpy()))\n",
        "print('Actual Label:', labels[1].numpy())#actual label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChZoq9dPSI9l",
        "colab_type": "text"
      },
      "source": [
        "## Challenge 2.2 \n",
        "\n",
        "Design a Deep Neural Network (and name it model4) for classification of images based on **[Fashion-MNIST](https://pytorch.org/docs/stable/torchvision/datasets.html#fashion-mnist)** dataset. Here the images are of dimension 28 * 28 and we have to classify them in one out of 10 categories.\n",
        "\n",
        "![alt text](https://github.com/zalandoresearch/fashion-mnist/blob/master/doc/img/fashion-mnist-sprite.png?raw=true)\n",
        "\n",
        "For more details **[click here](https://github.com/zalandoresearch/fashion-mnist)**. Your challenge is to:\n",
        "\n",
        "\n",
        "*   Load the dataset\n",
        "*   Build the network\n",
        "*  Train the network\n",
        "* Try to predict some random samples\n",
        "\n",
        "You can take the help of above code and make necessary changes to solve this challenge.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvwdgT_VmS3I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "8db8cb73-305a-4802-d8ce-a8a39311fc17"
      },
      "source": [
        "#load dataset in this cell"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to FashionMNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:01, 13628959.20it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting FashionMNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to FashionMNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 97076.76it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting FashionMNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to FashionMNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:01, 4069187.66it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting FashionMNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to FashionMNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 29542.89it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting FashionMNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9BNGOzlUV9H",
        "colab_type": "code",
        "outputId": "888217d6-40b5-4664-ffc7-aa5e74015b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# Build a feed-forward network"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=400, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=400, out_features=200, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=200, out_features=100, bias=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (7): Softmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFAjBs40wcDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specify loss and optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8J4BuAGUYuR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4156
        },
        "outputId": "c6be3df9-a60c-428a-f515-9fc7bbcca7c0"
      },
      "source": [
        "#train network in this cell"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10...  Loss: 2.3025\n",
            "Epoch: 1/10...  Loss: 2.3025\n",
            "Epoch: 1/10...  Loss: 2.3020\n",
            "Epoch: 1/10...  Loss: 2.3020\n",
            "Epoch: 1/10...  Loss: 2.3020\n",
            "Epoch: 1/10...  Loss: 2.3017\n",
            "Epoch: 1/10...  Loss: 2.3015\n",
            "Epoch: 1/10...  Loss: 2.3016\n",
            "Epoch: 1/10...  Loss: 2.3012\n",
            "Epoch: 1/10...  Loss: 2.3014\n",
            "Epoch: 1/10...  Loss: 2.3010\n",
            "Epoch: 1/10...  Loss: 2.3009\n",
            "Epoch: 1/10...  Loss: 2.3006\n",
            "Epoch: 1/10...  Loss: 2.3003\n",
            "Epoch: 1/10...  Loss: 2.3005\n",
            "Epoch: 1/10...  Loss: 2.3000\n",
            "Epoch: 1/10...  Loss: 2.3000\n",
            "Epoch: 1/10...  Loss: 2.2998\n",
            "Epoch: 1/10...  Loss: 2.2997\n",
            "Epoch: 1/10...  Loss: 2.2997\n",
            "Epoch: 1/10...  Loss: 2.2995\n",
            "Epoch: 1/10...  Loss: 2.2992\n",
            "Epoch: 1/10...  Loss: 2.2992\n",
            "Epoch: 2/10...  Loss: 1.2644\n",
            "Epoch: 2/10...  Loss: 2.2986\n",
            "Epoch: 2/10...  Loss: 2.2990\n",
            "Epoch: 2/10...  Loss: 2.2983\n",
            "Epoch: 2/10...  Loss: 2.2981\n",
            "Epoch: 2/10...  Loss: 2.2979\n",
            "Epoch: 2/10...  Loss: 2.2979\n",
            "Epoch: 2/10...  Loss: 2.2972\n",
            "Epoch: 2/10...  Loss: 2.2975\n",
            "Epoch: 2/10...  Loss: 2.2969\n",
            "Epoch: 2/10...  Loss: 2.2970\n",
            "Epoch: 2/10...  Loss: 2.2963\n",
            "Epoch: 2/10...  Loss: 2.2965\n",
            "Epoch: 2/10...  Loss: 2.2963\n",
            "Epoch: 2/10...  Loss: 2.2960\n",
            "Epoch: 2/10...  Loss: 2.2959\n",
            "Epoch: 2/10...  Loss: 2.2958\n",
            "Epoch: 2/10...  Loss: 2.2952\n",
            "Epoch: 2/10...  Loss: 2.2949\n",
            "Epoch: 2/10...  Loss: 2.2950\n",
            "Epoch: 2/10...  Loss: 2.2945\n",
            "Epoch: 2/10...  Loss: 2.2937\n",
            "Epoch: 2/10...  Loss: 2.2935\n",
            "Epoch: 3/10...  Loss: 0.2294\n",
            "Epoch: 3/10...  Loss: 2.2930\n",
            "Epoch: 3/10...  Loss: 2.2927\n",
            "Epoch: 3/10...  Loss: 2.2922\n",
            "Epoch: 3/10...  Loss: 2.2918\n",
            "Epoch: 3/10...  Loss: 2.2914\n",
            "Epoch: 3/10...  Loss: 2.2913\n",
            "Epoch: 3/10...  Loss: 2.2906\n",
            "Epoch: 3/10...  Loss: 2.2899\n",
            "Epoch: 3/10...  Loss: 2.2900\n",
            "Epoch: 3/10...  Loss: 2.2892\n",
            "Epoch: 3/10...  Loss: 2.2883\n",
            "Epoch: 3/10...  Loss: 2.2884\n",
            "Epoch: 3/10...  Loss: 2.2872\n",
            "Epoch: 3/10...  Loss: 2.2867\n",
            "Epoch: 3/10...  Loss: 2.2858\n",
            "Epoch: 3/10...  Loss: 2.2852\n",
            "Epoch: 3/10...  Loss: 2.2835\n",
            "Epoch: 3/10...  Loss: 2.2839\n",
            "Epoch: 3/10...  Loss: 2.2823\n",
            "Epoch: 3/10...  Loss: 2.2813\n",
            "Epoch: 3/10...  Loss: 2.2817\n",
            "Epoch: 3/10...  Loss: 2.2807\n",
            "Epoch: 3/10...  Loss: 2.2778\n",
            "Epoch: 4/10...  Loss: 1.4797\n",
            "Epoch: 4/10...  Loss: 2.2759\n",
            "Epoch: 4/10...  Loss: 2.2735\n",
            "Epoch: 4/10...  Loss: 2.2728\n",
            "Epoch: 4/10...  Loss: 2.2698\n",
            "Epoch: 4/10...  Loss: 2.2681\n",
            "Epoch: 4/10...  Loss: 2.2662\n",
            "Epoch: 4/10...  Loss: 2.2611\n",
            "Epoch: 4/10...  Loss: 2.2586\n",
            "Epoch: 4/10...  Loss: 2.2622\n",
            "Epoch: 4/10...  Loss: 2.2560\n",
            "Epoch: 4/10...  Loss: 2.2557\n",
            "Epoch: 4/10...  Loss: 2.2489\n",
            "Epoch: 4/10...  Loss: 2.2443\n",
            "Epoch: 4/10...  Loss: 2.2419\n",
            "Epoch: 4/10...  Loss: 2.2332\n",
            "Epoch: 4/10...  Loss: 2.2299\n",
            "Epoch: 4/10...  Loss: 2.2330\n",
            "Epoch: 4/10...  Loss: 2.2189\n",
            "Epoch: 4/10...  Loss: 2.2192\n",
            "Epoch: 4/10...  Loss: 2.2157\n",
            "Epoch: 4/10...  Loss: 2.2116\n",
            "Epoch: 4/10...  Loss: 2.2103\n",
            "Epoch: 5/10...  Loss: 0.4426\n",
            "Epoch: 5/10...  Loss: 2.1882\n",
            "Epoch: 5/10...  Loss: 2.1774\n",
            "Epoch: 5/10...  Loss: 2.1819\n",
            "Epoch: 5/10...  Loss: 2.1819\n",
            "Epoch: 5/10...  Loss: 2.1820\n",
            "Epoch: 5/10...  Loss: 2.1759\n",
            "Epoch: 5/10...  Loss: 2.1723\n",
            "Epoch: 5/10...  Loss: 2.1718\n",
            "Epoch: 5/10...  Loss: 2.1693\n",
            "Epoch: 5/10...  Loss: 2.1729\n",
            "Epoch: 5/10...  Loss: 2.1585\n",
            "Epoch: 5/10...  Loss: 2.1565\n",
            "Epoch: 5/10...  Loss: 2.1660\n",
            "Epoch: 5/10...  Loss: 2.1474\n",
            "Epoch: 5/10...  Loss: 2.1451\n",
            "Epoch: 5/10...  Loss: 2.1524\n",
            "Epoch: 5/10...  Loss: 2.1417\n",
            "Epoch: 5/10...  Loss: 2.1337\n",
            "Epoch: 5/10...  Loss: 2.1420\n",
            "Epoch: 5/10...  Loss: 2.1356\n",
            "Epoch: 5/10...  Loss: 2.1276\n",
            "Epoch: 5/10...  Loss: 2.1311\n",
            "Epoch: 5/10...  Loss: 2.1209\n",
            "Epoch: 6/10...  Loss: 1.5833\n",
            "Epoch: 6/10...  Loss: 2.1086\n",
            "Epoch: 6/10...  Loss: 2.1020\n",
            "Epoch: 6/10...  Loss: 2.1027\n",
            "Epoch: 6/10...  Loss: 2.1043\n",
            "Epoch: 6/10...  Loss: 2.0912\n",
            "Epoch: 6/10...  Loss: 2.0775\n",
            "Epoch: 6/10...  Loss: 2.0676\n",
            "Epoch: 6/10...  Loss: 2.0699\n",
            "Epoch: 6/10...  Loss: 2.0696\n",
            "Epoch: 6/10...  Loss: 2.0448\n",
            "Epoch: 6/10...  Loss: 2.0436\n",
            "Epoch: 6/10...  Loss: 2.0284\n",
            "Epoch: 6/10...  Loss: 2.0238\n",
            "Epoch: 6/10...  Loss: 2.0139\n",
            "Epoch: 6/10...  Loss: 2.0062\n",
            "Epoch: 6/10...  Loss: 1.9960\n",
            "Epoch: 6/10...  Loss: 2.0032\n",
            "Epoch: 6/10...  Loss: 1.9967\n",
            "Epoch: 6/10...  Loss: 1.9918\n",
            "Epoch: 6/10...  Loss: 1.9860\n",
            "Epoch: 6/10...  Loss: 1.9788\n",
            "Epoch: 6/10...  Loss: 1.9778\n",
            "Epoch: 7/10...  Loss: 0.5864\n",
            "Epoch: 7/10...  Loss: 1.9724\n",
            "Epoch: 7/10...  Loss: 1.9580\n",
            "Epoch: 7/10...  Loss: 1.9569\n",
            "Epoch: 7/10...  Loss: 1.9477\n",
            "Epoch: 7/10...  Loss: 1.9451\n",
            "Epoch: 7/10...  Loss: 1.9405\n",
            "Epoch: 7/10...  Loss: 1.9370\n",
            "Epoch: 7/10...  Loss: 1.9414\n",
            "Epoch: 7/10...  Loss: 1.9296\n",
            "Epoch: 7/10...  Loss: 1.9141\n",
            "Epoch: 7/10...  Loss: 1.9250\n",
            "Epoch: 7/10...  Loss: 1.9128\n",
            "Epoch: 7/10...  Loss: 1.9077\n",
            "Epoch: 7/10...  Loss: 1.9250\n",
            "Epoch: 7/10...  Loss: 1.9059\n",
            "Epoch: 7/10...  Loss: 1.9043\n",
            "Epoch: 7/10...  Loss: 1.8990\n",
            "Epoch: 7/10...  Loss: 1.9072\n",
            "Epoch: 7/10...  Loss: 1.8960\n",
            "Epoch: 7/10...  Loss: 1.8772\n",
            "Epoch: 7/10...  Loss: 1.8865\n",
            "Epoch: 7/10...  Loss: 1.8798\n",
            "Epoch: 7/10...  Loss: 1.8835\n",
            "Epoch: 8/10...  Loss: 1.5961\n",
            "Epoch: 8/10...  Loss: 1.8674\n",
            "Epoch: 8/10...  Loss: 1.8764\n",
            "Epoch: 8/10...  Loss: 1.8681\n",
            "Epoch: 8/10...  Loss: 1.8647\n",
            "Epoch: 8/10...  Loss: 1.8637\n",
            "Epoch: 8/10...  Loss: 1.8478\n",
            "Epoch: 8/10...  Loss: 1.8344\n",
            "Epoch: 8/10...  Loss: 1.8604\n",
            "Epoch: 8/10...  Loss: 1.8560\n",
            "Epoch: 8/10...  Loss: 1.8439\n",
            "Epoch: 8/10...  Loss: 1.8390\n",
            "Epoch: 8/10...  Loss: 1.8308\n",
            "Epoch: 8/10...  Loss: 1.8262\n",
            "Epoch: 8/10...  Loss: 1.8237\n",
            "Epoch: 8/10...  Loss: 1.8260\n",
            "Epoch: 8/10...  Loss: 1.8175\n",
            "Epoch: 8/10...  Loss: 1.8143\n",
            "Epoch: 8/10...  Loss: 1.8013\n",
            "Epoch: 8/10...  Loss: 1.8089\n",
            "Epoch: 8/10...  Loss: 1.8178\n",
            "Epoch: 8/10...  Loss: 1.8045\n",
            "Epoch: 8/10...  Loss: 1.7972\n",
            "Epoch: 9/10...  Loss: 0.7164\n",
            "Epoch: 9/10...  Loss: 1.7925\n",
            "Epoch: 9/10...  Loss: 1.7971\n",
            "Epoch: 9/10...  Loss: 1.7849\n",
            "Epoch: 9/10...  Loss: 1.7699\n",
            "Epoch: 9/10...  Loss: 1.7899\n",
            "Epoch: 9/10...  Loss: 1.7782\n",
            "Epoch: 9/10...  Loss: 1.7911\n",
            "Epoch: 9/10...  Loss: 1.7849\n",
            "Epoch: 9/10...  Loss: 1.7782\n",
            "Epoch: 9/10...  Loss: 1.7763\n",
            "Epoch: 9/10...  Loss: 1.7739\n",
            "Epoch: 9/10...  Loss: 1.7722\n",
            "Epoch: 9/10...  Loss: 1.7675\n",
            "Epoch: 9/10...  Loss: 1.7626\n",
            "Epoch: 9/10...  Loss: 1.7602\n",
            "Epoch: 9/10...  Loss: 1.7765\n",
            "Epoch: 9/10...  Loss: 1.7589\n",
            "Epoch: 9/10...  Loss: 1.7621\n",
            "Epoch: 9/10...  Loss: 1.7716\n",
            "Epoch: 9/10...  Loss: 1.7612\n",
            "Epoch: 9/10...  Loss: 1.7630\n",
            "Epoch: 9/10...  Loss: 1.7579\n",
            "Epoch: 9/10...  Loss: 1.7541\n",
            "Epoch: 10/10...  Loss: 1.6601\n",
            "Epoch: 10/10...  Loss: 1.7525\n",
            "Epoch: 10/10...  Loss: 1.7366\n",
            "Epoch: 10/10...  Loss: 1.7591\n",
            "Epoch: 10/10...  Loss: 1.7640\n",
            "Epoch: 10/10...  Loss: 1.7499\n",
            "Epoch: 10/10...  Loss: 1.7498\n",
            "Epoch: 10/10...  Loss: 1.7611\n",
            "Epoch: 10/10...  Loss: 1.7600\n",
            "Epoch: 10/10...  Loss: 1.7456\n",
            "Epoch: 10/10...  Loss: 1.7490\n",
            "Epoch: 10/10...  Loss: 1.7465\n",
            "Epoch: 10/10...  Loss: 1.7415\n",
            "Epoch: 10/10...  Loss: 1.7467\n",
            "Epoch: 10/10...  Loss: 1.7466\n",
            "Epoch: 10/10...  Loss: 1.7340\n",
            "Epoch: 10/10...  Loss: 1.7343\n",
            "Epoch: 10/10...  Loss: 1.7477\n",
            "Epoch: 10/10...  Loss: 1.7377\n",
            "Epoch: 10/10...  Loss: 1.7314\n",
            "Epoch: 10/10...  Loss: 1.7279\n",
            "Epoch: 10/10...  Loss: 1.7279\n",
            "Epoch: 10/10...  Loss: 1.7415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzEFat9ZUc8P",
        "colab_type": "code",
        "outputId": "2bd85e3e-f707-4b91-bc34-ff59f67c55fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "#make prediction(s) in this cell"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 784])\n",
            "Predicted Label: 5\n",
            "Actual Label: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATGst6rR2Fgq",
        "colab_type": "text"
      },
      "source": [
        "# 3. Computer Vision using CNN in PyTorch\n",
        "\n",
        "\n",
        "*   We will be creating a simple CNN for classifying images in **[CIFAR10 dataset](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar)**\n",
        "\n",
        "![CIFAR10](https://storage.googleapis.com/kaggle-competitions/kaggle/3649/media/cifar-10.png)\n",
        "\n",
        "*   The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class\n",
        "* There are 50000 training images and 10000 test images. \n",
        "\n",
        "\n",
        "**Loading and transforming the dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0mFMdFF6riZ",
        "colab_type": "code",
        "outputId": "c376d2f5-9a1c-4889-be76-c72586727348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = datasets.CIFAR10('CIFAR10/', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = datasets.CIFAR10(root='CIFAR10/', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=True, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 170483712/170498071 [00:23<00:00, 6829214.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQH9YTZJ9AlU",
        "colab_type": "text"
      },
      "source": [
        "**Creating CNN based Deep Neural Network having following layout**\n",
        "\n",
        "\n",
        "*   Input > Conv (ReLU) > MaxPool > Conv (ReLU) > MaxPool > FC (ReLU) > FC (ReLU) > FC (SoftMax) > 10 outputs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nCVPFp59Keq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)#in_channels, out_channels, kernel_size, \n",
        "        self.pool = nn.MaxPool2d(2,2)#2pool window of size 2, stride=2\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)#in_channels, out_channels, kernel_size, \n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)#in_features, out-features\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)#Flattening of tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "cnnModel = CNNModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAHpqAXg_M7v",
        "colab_type": "text"
      },
      "source": [
        "**Defining loss function and optimization algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzjbqXmN_Uph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(cnnModel.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMCPNZOv_kqx",
        "colab_type": "code",
        "outputId": "e67cf72d-ec17-4afa-e295-ac4c265d1ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  \n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels = data\n",
        "    \n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = cnnModel(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if i % 200 == 199:    # print every 200 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 200))\n",
        "      running_loss = 0.0\n",
        "print('Finished Training.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   200] loss: 2.304\n",
            "[1,   400] loss: 2.303\n",
            "[2,   200] loss: 2.300\n",
            "[2,   400] loss: 2.296\n",
            "[3,   200] loss: 2.274\n",
            "[3,   400] loss: 2.225\n",
            "[4,   200] loss: 2.092\n",
            "[4,   400] loss: 2.014\n",
            "[5,   200] loss: 1.931\n",
            "[5,   400] loss: 1.889\n",
            "[6,   200] loss: 1.807\n",
            "[6,   400] loss: 1.730\n",
            "[7,   200] loss: 1.666\n",
            "[7,   400] loss: 1.623\n",
            "[8,   200] loss: 1.582\n",
            "[8,   400] loss: 1.564\n",
            "[9,   200] loss: 1.524\n",
            "[9,   400] loss: 1.504\n",
            "[10,   200] loss: 1.485\n",
            "[10,   400] loss: 1.454\n",
            "Finished Training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uER4vTFA1Zm",
        "colab_type": "code",
        "outputId": "9a2d1ca6-b0b2-48eb-8bc0-c0bc3b7cc269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "cnnModel.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(testloader))\n",
        "batch_size = images.shape[0] #100 is batch size\n",
        "print(images.size())\n",
        "output = cnnModel.forward(images)#classification probabilities based on untrained model\n",
        "plt.imshow(images[1].numpy().transpose(1,2,0))\n",
        "print('Predicted Label:',classes[np.argmax(output[1].data.numpy())])\n",
        "print('Actual Label:', classes[labels[1].numpy()])#actual label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 3, 32, 32])\n",
            "Predicted Label: horse\n",
            "Actual Label: horse\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nT0ikUPyERMr/A95tEnrmHqykAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7D8HlhrH3VU",
        "colab_type": "text"
      },
      "source": [
        "## Challenge 3.1:\n",
        "Add a third convolution layer (input channel =16, output channel =16 and kernel size =5) and apply maxpooling (pooling window 2*2) on newly added layer. Retrain the network and make the predictons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkgV6fXbIMxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make changes in this cell\n",
        "class CNNModel2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)#in_channels, out_channels, kernel_size, \n",
        "        self.pool = nn.MaxPool2d(2, 2)#pool window size=2, stride=2 \n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)#in_channels, out_channels, kernel_size,\n",
        "        self.conv3 = nn.Conv2d('Fix Me', 'Fix Me', 'Fix Me')#in_channels=16, out_channels=15, kernel_size=5\n",
        "        self.fc1 = nn.Linear('Fix Me', 'Fix Me')#in_features=16*24*24, out-features=120\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = 'Fix Me'#first x will be passed to self.conv3() then to F.relu then to self.pool. See above statement\n",
        "        x = x.view('Fix Me', 'Fix Me')#Flattening of tensor => (-1, 16* 24* 24)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "cnnModel2 = CNNModel2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pjQDKqXYHfT",
        "colab_type": "text"
      },
      "source": [
        "Training the model will require considerable amount of time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK18gZWlKdvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make No Changes In This Cell\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(cnnModel2.parameters(), lr=0.01)\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  \n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels = data\n",
        "    \n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = cnnModel2(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if i % 200 == 199:    # print every 200 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 200))\n",
        "      running_loss = 0.0\n",
        "print('Finished Training.')\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "cnnModel.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(testloader))\n",
        "batch_size = images.shape[0] #100 is batch size\n",
        "print(images.size())\n",
        "output = cnnModel.forward(images)#classification probabilities based on untrained model\n",
        "plt.imshow(images[1].numpy().transpose(1,2,0))\n",
        "print('Predicted Label:',classes[np.argmax(output[1].data.numpy())])\n",
        "print('Actual Label:', classes[labels[1].numpy()])#actual label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8VKmqWPVTEe",
        "colab_type": "text"
      },
      "source": [
        "# 4. Validation and Avoiding Overfitting\n",
        "\n",
        "\n",
        "*   Unlike Tensorflow 2.x, PyTorch does not give us accuracy of our model automatically\n",
        "* We need to write code to specify how to compute accuracy for our model\n",
        "* We need to test accuracy for both our test data and train data\n",
        "* If accuracy is too high for train data and too low for test data then we are are encountering **overfitting**\n",
        "* Then we will have to apply overfitting solutions like increasing dataset, modifying model, dropout, regularization etc.\n",
        "  \n",
        "Creating a validation function to calculate Accuracy of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1RJhGVt8208",
        "colab_type": "code",
        "outputId": "620c1cd5-ef70-4161-86ae-aac76c792a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1815
        }
      },
      "source": [
        "# Implement a function for the validation pass\n",
        "def validation(model, testloader, criterion):\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    for images, labels in testloader:\n",
        "\n",
        "        images.resize_(images.shape[0], 784)\n",
        "\n",
        "        output = model.forward(images)\n",
        "        test_loss += criterion(output, labels).item()#item() gives us the scalar value held in the tensor\n",
        "\n",
        "        equality = (labels.data == output.max(dim=1)[1])#remember output is in shape [64,10]\n",
        "        accuracy += equality.type(torch.FloatTensor).mean()\n",
        "    \n",
        "    return test_loss, accuracy\n",
        "  \n",
        "loss,accuracy=validation(model,testloader,lossCriteria)\n",
        "print(\"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.2918, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3071, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3161, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3089, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3120, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3243, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3162, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3054, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3235, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3049, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2880, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2955, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3108, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3315, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3113, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2880, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3087, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3001, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3066, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3165, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3238, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3029, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3166, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3046, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3024, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3218, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3117, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3042, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3085, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3199, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3052, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3094, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3044, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3142, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3069, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3139, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3263, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3283, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2972, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3282, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3032, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3059, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3231, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3221, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3267, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3198, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3054, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3067, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3308, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2955, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3201, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2960, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3093, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3096, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2864, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3098, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3140, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3059, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3075, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3014, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3175, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3182, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2975, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2949, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3248, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3172, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3447, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3350, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2882, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3256, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3229, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3043, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3220, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3194, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2973, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3148, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3319, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3057, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3257, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3360, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3239, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2970, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2978, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3283, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3074, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3084, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3165, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2797, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3205, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3190, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2995, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3099, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3221, grad_fn=<NllLossBackward>)\n",
            "tensor(2.2887, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3255, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3264, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3004, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3081, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3126, grad_fn=<NllLossBackward>)\n",
            "tensor(2.3074, grad_fn=<NllLossBackward>)\n",
            "Test Accuracy: 0.102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEds2onoQjnd",
        "colab_type": "text"
      },
      "source": [
        "* Let us integrate above function in our training process to see real time accuracy of our model during training\n",
        "* Follwing cells show how to integrate our just defined validation function with training process of neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-O1Msu2JH49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unchanged Code of MNIST Classification discussed previously\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5, ), (0.5,)),# Subtract 0.5 from each pixel value and then divide it by 0.5 so as convert range of pixel value form -1 to +1\n",
        "                             ])\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
        "\n",
        "class Network(nn.Module):#specifying our network architecture\n",
        "  def __init__(self): #initializing the network\n",
        "    super().__init__()#it will call the init method of nn.Module\n",
        "    self.fc1 = nn.Linear(784,128)\n",
        "    self.fc2 = nn.Linear(128,64)\n",
        "    self.fc3 = nn.Linear(64,10)\n",
        "  \n",
        "  #all PyTorch Network must have forward method representing forward pass\n",
        "  def forward(self,x):#x is a tensor passed to forward function\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.softmax(x, dim=1)#The tensor will be of size 64(batcg size) * 10; we want to apply softmax to secomd dimension\n",
        "    return x   \n",
        "model = Network()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXvHUQ0QTW80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Changes will be made in this cell\n",
        "lossCriteria = nn.CrossEntropyLoss() #defining loss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) #lr is learning rate \n",
        "epochs = 10\n",
        "print_every = 500 #to print the current loss after every 500 steps\n",
        "steps = 0\n",
        "for e in range(epochs):\n",
        "    running_loss = 0 #total loss till in previous 'print_every'steps\n",
        "    for images, labels in iter(trainloader):\n",
        "        steps += 1\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images.resize_(images.shape[0], 784)\n",
        "        \n",
        "        optimizer.zero_grad()#!!!VERY IMPORTANT. To ensure for each backpropagation, new gradients are computed\n",
        "        \n",
        "        # Forward and backward passes\n",
        "        output = model.forward(images)\n",
        "        loss = lossCriteria(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()#update weights\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if steps % print_every == 0:\n",
        "            \n",
        "            #Checking accuracy and loss\n",
        "            with torch.no_grad():#to speed up the process\n",
        "                test_loss, accuracy = validation(model, testloader, lossCriteria)#to test accuracy of model on train data\n",
        "                \n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
        "                  \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),#display test loss\n",
        "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))#display test accuracy\n",
        "            running_loss=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHzxPCv9Vl8d",
        "colab_type": "text"
      },
      "source": [
        "## Challenge 4.1\n",
        "Find out the final  test accuracy and test loss of the models you created in Challenge 2.1 and 2.2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlESZ8xBWdw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for model created in Challenge 2.1\n",
        "loss2_1,accuracy2_1=validation('Replace With Model (Remove single quotes)','Replace With Test Loader(Remove single quotes)','Replace With Loss Criteria (Remove single quotes)')\n",
        "print(\"Test Loss:  {:.3f}.. \".format(loss2_1/len('Replace With Test Loader(Remove single quotes)'))\n",
        "print(\"Test Accuracy: {:.3f}\".format(accuracy2_1/len('Replace With Test Loader(Remove single quotes)')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cU4Uz4XXlKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for model created in Challenge 2.2\n",
        "loss2_2,accuracy2_2=validation('Replace With Model (Remove single quotes)','Replace With Test Loader(Remove single quotes)','Replace With Loss Criteria (Remove single quotes)')\n",
        "print(\"Test Loss: {:.3f}.. \".format(loss2_2/len('Replace With Test Loader(Remove single quotes)'))\n",
        "print(\"Test Accuracy: {:.3f}\".format(accuracy2_2/len('Replace With Test Loader(Remove single quotes)')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfMecQx2d9vi",
        "colab_type": "text"
      },
      "source": [
        "# 5. Saving and Loading Trained Networks in PyTorch\n",
        "\n",
        "Saving the model involves two things\n",
        "\n",
        "*   Saving the state of the model (weights and bias). \n",
        "* This is is useful when we dont want to retrain the model again and again\n",
        "* Instead we can reload the state of model in future\n",
        "* The parameters (weights, biases etc) are stored in model's **state_dict**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzW898_gjDOn",
        "colab_type": "code",
        "outputId": "e288191f-af4c-4539-cee4-5b855df879de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "print(\"Model under consideration: \",model)\n",
        "print(\"The keys of state dictionary \", model.state_dict().keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model under consideration:  Network(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "The keys of state dictionary  odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uNSZ3QIJ1UK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The values of state dictionary \", model.state_dict().values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Ok7Bk8KuvZ",
        "colab_type": "text"
      },
      "source": [
        "**Saving the state_dict():**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNYK_ui5IrS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmt4rNOyLDi-",
        "colab_type": "text"
      },
      "source": [
        "Ensure that the state of model is actually saved by selecting left menu then selecting File and then selecting refresh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2UxSFjqLtFN",
        "colab_type": "text"
      },
      "source": [
        "**Loading the pre trained model using saved_dict:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAxL5jGnLBBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_dict = torch.load('checkpoint.pth')\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7EB4s0zL5sL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_reloaded=Network() #create a new model\n",
        "model_reloaded.load_state_dict(state_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjilOGfy-tNS",
        "colab_type": "text"
      },
      "source": [
        "Ignore if you see a message 'IncompatibleKeys(missing_keys=[], unexpected_keys=[])'. \n",
        "\n",
        "**Making predictions with the newly model with pretrained wieghts and biases:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzi6UbJzM0YY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_reloaded.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(trainloader))\n",
        "batch_size = images.shape[0] #64 is batch size\n",
        "images.resize_(batch_size, 784)#in_place resizing of image\n",
        "print(images.size())\n",
        "output = model.forward(images)#classification probabilities based on untrained model\n",
        "print('Predicted Label:',np.argmax(output[1].data.numpy()))\n",
        "print('Actual Label:', labels[1].numpy())#actual label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeQMuO3GALRs",
        "colab_type": "text"
      },
      "source": [
        "## Challenge 5.1\n",
        "Save the model that you created in Challenge 2.1 and reload the model and make the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn8CdtNkAbbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pE2Opo0_-XZ",
        "colab_type": "text"
      },
      "source": [
        "# 6. Transfer Learning Using PyTorch\n",
        "\n",
        "\n",
        "*   We will modify a pre-trained model to classify images as cats or dogs\n",
        "*   There are various pre-trained models available in PyTorch via **[torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html)** that are already trained on massive datasets such as ImageNet\n",
        "* The pre-trained models are good feature detectors that can be used as an input for simple feed forward classifiers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgWmi-WqjQIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#An additional import is required\n",
        "from torchvision import models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjBFveOzPYzI",
        "colab_type": "text"
      },
      "source": [
        "We w to"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAWbyn_OPZI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://files.fast.ai/data/dogscats.zip\n",
        "!unzip -q dogscats.zip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7a14HMoEHJo",
        "colab_type": "text"
      },
      "source": [
        "The pretrained model that we will be using is **[DenseNet](https://pytorch.org/docs/stable/torchvision/models.html#id5)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J1x_ifREVM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.densenet121(pretrained=True)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKTc_NkGDB5j",
        "colab_type": "text"
      },
      "source": [
        "**The input that the network expects:**\n",
        "\n",
        "*   All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W)\n",
        "* Here H and W are expected to be at least 224\n",
        "*   The images have to be loaded in to a range of [0, 1] \n",
        "* Then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AqHFhloCcr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'dogscats'\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])])\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=transform)\n",
        "test_data = datasets.ImageFolder(data_dir + '/valid', transform=transform)\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=64,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWXW3vokEoKv",
        "colab_type": "text"
      },
      "source": [
        "In Transfer Learning, normally the model has two parts:\n",
        "\n",
        "\n",
        "*   **Features Part:** Where the model extract the features of the data. This part normally remains unchanged\n",
        "*   **Classifier Part:** Here the model performs classification. This part needs to be updated as per the given problem\n",
        "\n",
        "We need to train only the updated classifier part on our dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2ybg2bBIN8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False#this ensures that model will not get trained during training\n",
        "\n",
        "from collections import OrderedDict\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(1024, 500)),#The first value must be same as output of classifier\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('fc2', nn.Linear(500, 2)),\n",
        "                          ('output', nn.Softmax(dim=1))\n",
        "                          ]))\n",
        "    \n",
        "model.classifier = classifier #This updates the classifier part of the model with newly created classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGca1ibDSc_w",
        "colab_type": "text"
      },
      "source": [
        "Training classifier part of our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdizsf8FSbLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Only train the classifier parameters, feature parameters are frozen\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "for ii, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        outputs = model.forward(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()#update weights\n",
        "\n",
        "        if ii==3:#only training on 3 batches\n",
        "            break\n",
        "print(\"Training on CPU takes: {}\".format((time.time()-start)/3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTEVrrvDKI0B",
        "colab_type": "text"
      },
      "source": [
        "* For training our transfer learning based  model we also  can **levarage the power of GPU**:\n",
        "* Training our model on GPU results in significant decrease in training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59U9d2R6VWGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Only train the classifier parameters, feature parameters are frozen\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "model.to(device)#moving model to GPU (if available)\n",
        "\n",
        "for ii, (inputs, labels) in enumerate(trainloader):\n",
        "  # Move input and label tensors to the GPU \n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  outputs = model.forward(inputs)\n",
        "  loss = criterion(outputs, labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()#update weights\n",
        "\n",
        "  if ii==3:#only training on 3 batches\n",
        "    break\n",
        "print(f\"Device = {device}; Time per batch: {(time.time() - start)/3:.3f} seconds\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4_uC3fCZ7NJ",
        "colab_type": "text"
      },
      "source": [
        "Doing Classification with our transfer learning based model for Cats Vs Dogs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sodIL5ZQDlyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(testloader))\n",
        "print(images.size())\n",
        "output = model.forward(images)\n",
        "plt.imshow(images[50].numpy().transpose(1,2,0))\n",
        "print('Predicted Label:',np.argmax(output[50].data.numpy()))#1 is dog 0 is cat\n",
        "print('Actual Label:', labels[50].numpy())#actual label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSTjY6a3k2kA",
        "colab_type": "text"
      },
      "source": [
        "## Solution: Challenge 2.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEAR2S4Pk-UO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load dataset in this cell\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5, ), (0.5,)) # Subtract 0.5 from each pixel value and then divide it by 0.5 so as convert range of pixel value form -1 to +1\n",
        "                             ])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('FashionMNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('FashionMNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eodrgqAElX93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a feed-forward network\n",
        "#Fix Me - Add layers here. Take help from above cells\n",
        "input_size=784\n",
        "output_size=10\n",
        "model4 = nn.Sequential(nn.Linear(input_size, 400),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(400, 200),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(200, 100),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(100,output_size),\n",
        "                      nn.Softmax(dim=1))\n",
        "print(model4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLtNbfyQlhKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specify loss and optimizer\n",
        "lossCriteria = nn.CrossEntropyLoss() #defining loss\n",
        "optimizer = torch.optim.SGD(model4.parameters(), lr=0.005) #lr is learning rate "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73jjc30Hlnf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train network in this cell\n",
        "train_loss=[]\n",
        "epochs = 10\n",
        "print_every = 40 #to print the current loss after every 40 steps\n",
        "steps = 0\n",
        "for e in range(epochs):\n",
        "    running_loss = 0 #total loss till in previous 'print_every'steps\n",
        "    for images, labels in iter(trainloader):\n",
        "        steps += 1\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images.resize_(images.shape[0], 784)\n",
        "        \n",
        "        optimizer.zero_grad()#!!!VERY IMPORTANT. To ensure for each backpropagation, new gradients are computed\n",
        "        \n",
        "        # Forward and backward passes\n",
        "        output = model4.forward(images)\n",
        "        loss = lossCriteria(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()#update weights\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        train_loss.append(loss)\n",
        "        if steps % print_every == 0:\n",
        "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
        "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
        "            \n",
        "            running_loss = 0\n",
        "plt.plot(train_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAD7kqbFl1XC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make prediction(s) in this cell\n",
        "#Do not change anything in this cell\n",
        "#Testing predictions of trained model\n",
        "model4.eval() # to tell PyTorch that we are in evaluation mode so batchnorm or dropout layers will work in eval mode instead of training mode.\n",
        "images , labels = next(iter(testloader))\n",
        "batch_size = images.shape[0] #64 is batch size\n",
        "images.resize_(batch_size, 784)#in_place resizing of image\n",
        "print(images.size())\n",
        "prob = model4.forward(images)#classification probabilities based on untrained model\n",
        "print('Predicted Label:',np.argmax(prob[1].data.numpy()))\n",
        "print('Actual Label:', labels[1].numpy())#actual label"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}